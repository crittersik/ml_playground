{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "form https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/3_regularization.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in `1_notmnist.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "data_root = 'notMNIST/'\n",
    "pickle_file = 'notMNIST.pickle'\n",
    "path = os.path.join(data_root, pickle_file)\n",
    "\n",
    "\n",
    "with open(path, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    train_dataset = save['train_dataset']\n",
    "    train_labels = save['train_labels']\n",
    "    valid_dataset = save['valid_dataset']\n",
    "    valid_labels = save['valid_labels']\n",
    "    test_dataset = save['test_dataset']\n",
    "    test_labels = save['test_labels']\n",
    "    del save  # hint to help gc free up memory\n",
    "    print('Training set', train_dataset.shape, train_labels.shape)\n",
    "    print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "    print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "    dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "    # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
    "    labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization in logistic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# see code from 2_fullyconnected.ipynb\n",
    "batch_size = 128\n",
    "hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    weights = tf.Variable(tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "    beta = tf.placeholder(tf.float32)  # regularization term\n",
    "\n",
    "    logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "    regularization = beta * (tf.nn.l2_loss(weights) + tf.nn.l2_loss(biases))\n",
    "\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)\n",
    "    ) + regularization\n",
    "\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 16.174650\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 14.2%\n",
      "\n",
      "Minibatch loss at step 200: 3.413007\n",
      "Minibatch accuracy: 67.2%\n",
      "Validation accuracy: 72.0%\n",
      "\n",
      "Minibatch loss at step 400: 2.527621\n",
      "Minibatch accuracy: 72.7%\n",
      "Validation accuracy: 75.0%\n",
      "\n",
      "Minibatch loss at step 600: 2.449730\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 76.1%\n",
      "\n",
      "Minibatch loss at step 800: 1.844812\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 76.7%\n",
      "\n",
      "Test accuracy: 84.5%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 1000\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta:0.0005}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 200 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "            print(\"\")\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "beta_vals = np.logspace(-4, 0, num = 20)\n",
    "validation_acc = []\n",
    "\n",
    "for beta_val in beta_vals:\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.global_variables_initializer().run()\n",
    "        for step in range(num_steps):\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    \n",
    "            feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta : beta_val} \n",
    "            _, l, predictions = session.run(\n",
    "                [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "            \n",
    "        validation_acc.append(accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEECAYAAADTdnSRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9c412f4d30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_beta(beta_vals, validation_acc):\n",
    "    plt.semilogx(beta_vals, validation_acc)\n",
    "    plt.show()\n",
    "\n",
    "plot_beta(beta_vals, validation_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization in neural model with a hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    beta = tf.placeholder(tf.float32)\n",
    "\n",
    "    # Variables.\n",
    "    weights_1 = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_nodes]))\n",
    "    biases_1 = tf.Variable(tf.zeros([hidden_nodes]))\n",
    "    \n",
    "    weights_2 = tf.Variable(tf.truncated_normal([hidden_nodes, num_labels]))\n",
    "    biases_2 = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    # Regularization\n",
    "    regularization = beta * (tf.nn.l2_loss(weights_1) + tf.nn.l2_loss(biases_1) + tf.nn.l2_loss(weights_2) + tf.nn.l2_loss(biases_2))\n",
    "    \n",
    "    # Training computation.\n",
    "    def compute(input_data):       \n",
    "        layer_1 = tf.nn.relu(tf.matmul(input_data, weights_1) + biases_1)\n",
    "        return tf.matmul(layer_1, weights_2) + biases_2\n",
    "    \n",
    "    logits = compute(tf_train_dataset)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)) + regularization\n",
    "    \n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(compute(tf_valid_dataset))\n",
    "    test_prediction = tf.nn.softmax(compute(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 495.111389\n",
      "Minibatch accuracy: 10.9%\n",
      "Validation accuracy: 38.9%\n",
      "\n",
      "Test accuracy: 85.0%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta:0.0005}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 200 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "            print(\"\")\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "beta_vals = np.logspace(-4, 0, num = 20)\n",
    "validation_acc = []\n",
    "\n",
    "for beta_val in beta_vals:\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.global_variables_initializer().run()\n",
    "        for step in range(num_steps):\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    \n",
    "            feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta : beta_val} \n",
    "            _, l, predictions = session.run(\n",
    "                [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "            \n",
    "        validation_acc.append(accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEECAYAAADTdnSRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRCSG/g+Uy+k9rZSlmwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9c15a79320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_beta(beta_vals, validation_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 576.509521\n",
      "Minibatch accuracy: 11.7%\n",
      "Validation accuracy: 23.5%\n",
      "\n",
      "Minibatch loss at step 10: 230.903320\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 70.6%\n",
      "\n",
      "Test accuracy: 78.4%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 11\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta : 0.0005}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 10 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "            print(\"\")\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    dropout_proba = tf.placeholder(tf.float32)\n",
    "    \n",
    "    # Variables.\n",
    "    weights_1 = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_nodes]))\n",
    "    biases_1 = tf.Variable(tf.zeros([hidden_nodes]))\n",
    "    \n",
    "    weights_2 = tf.Variable(tf.truncated_normal([hidden_nodes, num_labels]))\n",
    "    biases_2 = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    layer_1 = tf.nn.relu(tf.matmul(tf_train_dataset, weights_1) + biases_1)\n",
    "    layer_1_drop = tf.nn.dropout(layer_1, dropout_proba)\n",
    "    logits = tf.matmul(layer_1_drop, weights_2) + biases_2\n",
    "    \n",
    "     \n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    # Testing and validation computation.\n",
    "    def compute(input_data):       \n",
    "        layer_1 = tf.nn.relu(tf.matmul(input_data, weights_1) + biases_1)\n",
    "        return tf.matmul(layer_1, weights_2) + biases_2\n",
    "    \n",
    "    valid_prediction = tf.nn.softmax(compute(tf_valid_dataset))\n",
    "    test_prediction = tf.nn.softmax(compute(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 559.223633\n",
      "Minibatch accuracy: 7.8%\n",
      "Validation accuracy: 34.6%\n",
      "\n",
      "Minibatch loss at step 200: 49.120789\n",
      "Minibatch accuracy: 69.5%\n",
      "Validation accuracy: 80.1%\n",
      "\n",
      "Minibatch loss at step 400: 40.094555\n",
      "Minibatch accuracy: 72.7%\n",
      "Validation accuracy: 80.9%\n",
      "\n",
      "Minibatch loss at step 600: 29.943146\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 80.5%\n",
      "\n",
      "Minibatch loss at step 800: 24.538130\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 79.2%\n",
      "\n",
      "Minibatch loss at step 1000: 9.512255\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 79.8%\n",
      "\n",
      "Test accuracy: 87.0%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 1001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, dropout_proba : 0.5}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 200 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "            print(\"\")\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 417.114349\n",
      "Minibatch accuracy: 11.7%\n",
      "Validation accuracy: 33.1%\n",
      "\n",
      "Minibatch loss at step 10: 261.704346\n",
      "Minibatch accuracy: 60.9%\n",
      "Validation accuracy: 69.1%\n",
      "\n",
      "Minibatch loss at step 20: 88.264717\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 78.1%\n",
      "\n",
      "Test accuracy: 85.4%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 21\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, dropout_proba : 0.5}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 10 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "            print(\"\")\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    dropout_proba = tf.placeholder(tf.float32)\n",
    "    \n",
    "    # Variables.\n",
    "    weights_1 = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_nodes], stddev=0.1))\n",
    "    biases_1 = tf.Variable(tf.zeros([hidden_nodes]))\n",
    "    weights_2 = tf.Variable(tf.truncated_normal([hidden_nodes, hidden_nodes], stddev=0.1))\n",
    "    biases_2 = tf.Variable(tf.zeros([hidden_nodes]))\n",
    "    weights_3 = tf.Variable(tf.truncated_normal([hidden_nodes, hidden_nodes], stddev=0.1))\n",
    "    biases_3 = tf.Variable(tf.zeros([hidden_nodes]))\n",
    "    weights_4 = tf.Variable(tf.truncated_normal([hidden_nodes, num_labels], stddev=0.1))\n",
    "    biases_4 = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "\n",
    "    layer_1 = tf.nn.relu(tf.matmul(tf_train_dataset, weights_1) + biases_1)\n",
    "    layer_1 = tf.nn.dropout(layer_1, dropout_proba)\n",
    "\n",
    "    layer_2 = tf.nn.relu(tf.matmul(layer_1, weights_2) + biases_2)\n",
    "    layer_2 = tf.nn.dropout(layer_2, dropout_proba)\n",
    "    \n",
    "    layer_3 = tf.nn.relu(tf.matmul(layer_2, weights_3) + biases_3)\n",
    "    layer_3 = tf.nn.dropout(layer_3, dropout_proba)\n",
    "    \n",
    "    logits = tf.matmul(layer_3, weights_4) + biases_4\n",
    "     \n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    # global_step = tf.Variable(0)\n",
    "    # learning_rate = tf.train.exponential_decay(0.2, global_step, 10000, 0.96)\n",
    "    # optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.05, step, 100000, 0.96, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    # Testing and validation computation.\n",
    "    def compute(input_data):       \n",
    "        layer_1 = tf.nn.relu(tf.matmul(input_data, weights_1) + biases_1)\n",
    "        layer_2 = tf.nn.relu(tf.matmul(layer_1, weights_2) + biases_2)\n",
    "        layer_3 = tf.nn.relu(tf.matmul(layer_2, weights_3) + biases_3)\n",
    "        return tf.nn.relu(tf.matmul(layer_3, weights_4) + biases_4)\n",
    "    \n",
    "    valid_prediction = tf.nn.softmax(compute(tf_valid_dataset))\n",
    "    test_prediction = tf.nn.softmax(compute(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 40.900890\n",
      "Minibatch accuracy: 14.1%\n",
      "Validation accuracy: 18.0%\n",
      "\n",
      "Minibatch loss at step 500: 0.873099\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 80.1%\n",
      "\n",
      "Minibatch loss at step 1000: 0.781923\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 81.0%\n",
      "\n",
      "Minibatch loss at step 1500: 0.720385\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 81.9%\n",
      "\n",
      "Minibatch loss at step 2000: 0.624431\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 82.4%\n",
      "\n",
      "Minibatch loss at step 2500: 0.609813\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 82.8%\n",
      "\n",
      "Minibatch loss at step 3000: 0.729615\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 83.1%\n",
      "\n",
      "Minibatch loss at step 3500: 0.935477\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 83.3%\n",
      "\n",
      "Minibatch loss at step 4000: 0.709962\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 83.6%\n",
      "\n",
      "Minibatch loss at step 4500: 0.691989\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 83.6%\n",
      "\n",
      "Minibatch loss at step 5000: 0.538987\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 83.6%\n",
      "\n",
      "Minibatch loss at step 5500: 0.657160\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 83.9%\n",
      "\n",
      "Minibatch loss at step 6000: 0.382528\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 83.6%\n",
      "\n",
      "Minibatch loss at step 6500: 0.810781\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 84.0%\n",
      "\n",
      "Minibatch loss at step 7000: 0.524322\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 84.1%\n",
      "\n",
      "Minibatch loss at step 7500: 0.680080\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 84.2%\n",
      "\n",
      "Minibatch loss at step 8000: 0.491864\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 84.3%\n",
      "\n",
      "Test accuracy: 90.9%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 8001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, dropout_proba : 0.5}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "            print(\"\")\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
