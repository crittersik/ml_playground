{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d79eb9e-d7b7-4306-963f-ac54c1cb43e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this tutorial we will write from scratch a simple transformer model\n",
    "# with single-head attention layer, using Tensorflow\n",
    "\n",
    "# X - input matrix, dimension n * d_model, n is the number of tokens\n",
    "# We compute Query, Key and Value matrices,\n",
    "# Query = X * W_Q\n",
    "# Key = X * W_K\n",
    "# Value = X * W_V\n",
    "# W_{Q,L,V} - learned weight matrices\n",
    "# Attention(Q,K,V) = softmax(Query * Key^T / \\sqrt(d_k) ) * Value\n",
    "# Query * Key^T - computing pairwise similarity scores between tokens\n",
    "# / \\sqrt(d_k) - normalization (large vector effect)\n",
    "# softmax - transform scores into probabilities\n",
    "# * Value - weights values by attention scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a3db9118-c2f4-4fdc-ace5-d9246ef70fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ad02d6f3-2645-4232-a68f-d3b7369543ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_k, **kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model: Dimensionality of the input embeddings.\n",
    "            d_k: Dimensionality for the queries, keys, and values.\n",
    "                 ---\n",
    "                 The core operation in attention is the dot product \n",
    "                 between queries and keys. For this dot product to be valid, \n",
    "                 the query and key vectors must have the same dimension. \n",
    "                 ---\n",
    "                 Alternatively, we would require additional transformation\n",
    "                 to align dimensions.\n",
    "        \"\"\"\n",
    "        super(SingleHeadAttention, self).__init__(**kwargs)\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_k\n",
    "\n",
    "        # Linear layers for Query, Key, and Value\n",
    "        # TF Dense layer computes a transformation of the form: y=XW+b\n",
    "        # Which is exactly what we are aiming to do\n",
    "        self.wq = tf.keras.layers.Dense(d_k)\n",
    "        self.wk = tf.keras.layers.Dense(d_k)\n",
    "        self.wv = tf.keras.layers.Dense(d_k)\n",
    "        # Optional final dense layer to project back to d_model dimension\n",
    "        # It's more important for cases with mutliple attention heads, which require \n",
    "        # concatenation of outputs.\n",
    "        # We will leave it for completeness\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, seq_length, d_model)\n",
    "        Returns:\n",
    "            output: Tensor of shape (batch_size, seq_length, d_model)\n",
    "            attention_weights: Tensor of shape (batch_size, seq_length, seq_length)\n",
    "        \"\"\"\n",
    "        # Compute Q, K, V matrices\n",
    "        Q = self.wq(x)  # (batch_size, seq_length, d_k)\n",
    "        K = self.wk(x)  # (batch_size, seq_length, d_k)\n",
    "        V = self.wv(x)  # (batch_size, seq_length, d_k)\n",
    "\n",
    "        # Compute the dot products between Q and K^T\n",
    "        # scores shape: (batch_size, seq_length, seq_length)\n",
    "        scores = tf.matmul(Q, K, transpose_b=True)\n",
    "\n",
    "        # Scale the scores\n",
    "        scores = scores / math.sqrt(self.d_k)\n",
    "\n",
    "        # Apply softmax to get the attention weights\n",
    "        attention_weights = tf.nn.softmax(scores, axis=-1)\n",
    "\n",
    "        # Multiply the attention weights by the V matrix to get the output\n",
    "        attention_output = tf.matmul(attention_weights, V)  # (batch_size, seq_length, d_k)\n",
    "\n",
    "        # Optionally, project the output back to d_model dimension\n",
    "        output = self.dense(attention_output)  # (batch_size, seq_length, d_model)\n",
    "        return output, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cefe05f6-9b64-41a1-a7f0-4bb2eed2ad69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input tf.Tensor(\n",
      "[[[0.67898333 0.65998733 0.0076412  0.68809557 0.04945338 0.71485984\n",
      "   0.21802402 0.4134797  0.7995225  0.97210467 0.47264004 0.10039985\n",
      "   0.8632512  0.80847824 0.62963307 0.46375966]\n",
      "  [0.26390696 0.31222498 0.61185145 0.41408443 0.685186   0.46431303\n",
      "   0.46394253 0.24405694 0.08512807 0.0776546  0.00355649 0.815848\n",
      "   0.17615402 0.2074641  0.81066537 0.9257114 ]\n",
      "  [0.6281488  0.17580938 0.1155225  0.41840672 0.9343326  0.2072035\n",
      "   0.8145603  0.4319359  0.1788615  0.06442642 0.18461561 0.07547843\n",
      "   0.44993734 0.10015655 0.7201687  0.35097647]]\n",
      "\n",
      " [[0.13236713 0.55928576 0.2678516  0.07935917 0.5479156  0.85862076\n",
      "   0.09885895 0.5835713  0.56326807 0.82210815 0.2779752  0.4260409\n",
      "   0.16854334 0.6837851  0.585641   0.7409786 ]\n",
      "  [0.37479138 0.84219646 0.7647048  0.23238194 0.12509525 0.47940254\n",
      "   0.08150017 0.5011556  0.1554234  0.48021924 0.34506392 0.3762164\n",
      "   0.37518096 0.06120658 0.02597237 0.7343701 ]\n",
      "  [0.09509385 0.3911016  0.8239187  0.17954838 0.74511886 0.24153066\n",
      "   0.5750226  0.26599133 0.57638085 0.31044245 0.68504477 0.6249143\n",
      "   0.70009995 0.958557   0.8147141  0.70171523]]], shape=(2, 3, 16), dtype=float32)\n",
      "Output shape: (2, 3, 16)\n",
      "Attention weights: tf.Tensor(\n",
      "[[[0.24258941 0.3838074  0.37360322]\n",
      "  [0.27928272 0.38376406 0.33695328]\n",
      "  [0.30246148 0.3672926  0.3302459 ]]\n",
      "\n",
      " [[0.28275    0.30027133 0.41697863]\n",
      "  [0.2785248  0.26203388 0.4594413 ]\n",
      "  [0.2878297  0.29708788 0.41508242]]], shape=(2, 3, 3), dtype=float32)\n",
      "Attention weights shape: (2, 3, 3)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "seq_length = 3\n",
    "d_model = 16\n",
    "d_k = 8\n",
    "\n",
    "# Create dummy input data\n",
    "dummy_input = tf.random.uniform((batch_size, seq_length, d_model))\n",
    "print(\"input\", dummy_input)\n",
    "\n",
    "# Initialize and call the attention layer\n",
    "attention_layer = SingleHeadAttention(d_model, d_k)\n",
    "output, attn_weights = attention_layer(dummy_input)\n",
    "\n",
    "print(\"Output shape:\", output.shape) # Expected: (batch_size, seq_length, d_model)\n",
    "print(\"Attention weights:\", attn_weights) \n",
    "print(\"Attention weights shape:\", attn_weights.shape) # Expected: (batch_size, seq_length, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68b7b9b-92f0-45a9-8e63-965f9f97b461",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
