{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is Assignment 1 of the Udacity course on Deep Learning\n",
    "\n",
    "Preprocess notMNIST data and train a simple logistic regression model on it\n",
    "\n",
    "Data are read and pickled using code available here:\n",
    "\n",
    "For details see: \n",
    "\n",
    "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/1_notmnist.ipynb\n",
    "\n",
    "I add this notebook for completeness to my repo too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from IPython.display import display, Image\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Display random image from chosen dir\n",
    "\n",
    "data_root = 'notMNIST/'\n",
    "choose_dir = 'notMNIST_small/A'\n",
    "path_to_dir = os.path.join(data_root, choose_dir)\n",
    "images = os.listdir(path_to_dir)\n",
    "display(Image(filename=os.path.join(path_to_dir, random.choice(images))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read pickled datasets\n",
    "\n",
    "with open(os.path.join(data_root, 'notMNIST.pickle'), 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    \n",
    "train_dataset = data['train_dataset']\n",
    "train_labels = data['train_labels']\n",
    "valid_dataset = data['valid_dataset']\n",
    "valid_labels= data['valid_labels']\n",
    "test_dataset = data['test_dataset']\n",
    "test_labels = data['test_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check if it still looks good after pickling using matplotlib\n",
    "\n",
    "sample_image = random.choice(train_dataset)\n",
    "plt.imshow(sample_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check for overlapping samples\n",
    "\n",
    "def count_overlap(dataset_1, dataset_2):\n",
    "    start = time.clock()\n",
    "    hashed_1 = {hash(tuple(image.ravel())) for image in dataset_1}\n",
    "    hashed_2 = {hash(tuple(image.ravel())) for image in dataset_2}\n",
    "    n_overlaps = len(set(hashed_1).intersection(hashed_2))\n",
    "    return n_overlaps, time.clock()-start\n",
    "\n",
    "c1 = count_overlap(train_dataset, valid_dataset)\n",
    "c2 = count_overlap(valid_dataset, test_dataset)\n",
    "c3 = count_overlap(train_dataset, test_dataset)\n",
    "\n",
    "print('overlap between train and valid {0:06d} evaluated in {0:.3f}'.format(c1[0], c1[1]))\n",
    "print('overlap between valid and test {0:06d} evaluated in {0:.3f}'.format(c2[0], c2[1]))\n",
    "print('overlap between train and test {0:06d} evaluated in {0:.3f}'.format(c3[0], c3[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def there(l, img):\n",
    "    return (l,tuple(img.ravel()))\n",
    "\n",
    "def back(tup, size_x=28, size_y=28):\n",
    "    return tup[0], np.array(tup[1]).reshape((size_x, size_y))\n",
    "\n",
    "# Clear from overlapping samples\n",
    "def clear_overlap(dataset, labels, dataset_to_clear, labels_to_clear):\n",
    "    start = time.clock()\n",
    "    set_1 = {there(l, img) for img, l in zip(dataset, labels)}\n",
    "    set_to_clear = {there(l, img) for img, l in zip(dataset_to_clear, labels_to_clear)}\n",
    "    cleared_aux = set_to_clear.difference(set_1)\n",
    "    print('clearing took {}'.format(time.clock()-start))\n",
    "    dataset_cleared = np.array([back(elem)[1] for elem in cleared_aux])\n",
    "    labels_cleared = np.array([back(elem)[0] for elem in cleared_aux])\n",
    "    return dataset_cleared, labels_cleared\n",
    "\n",
    "                                     \n",
    "valid_dataset_c, valid_labels_c = clear_overlap(\n",
    "    train_dataset, train_labels, valid_dataset, valid_labels,\n",
    ")\n",
    "\n",
    "test_dataset_c, test_labels_c = clear_overlap(\n",
    "    valid_dataset, valid_labels, test_dataset, test_labels,\n",
    ")\n",
    "test_dataset_c, test_labels_c = clear_overlap(\n",
    "    train_dataset, train_labels, test_dataset_c, test_labels_c,\n",
    ")\n",
    "\n",
    "print('Compare sizes of cleared datasets')\n",
    "print(valid_dataset.shape[0], 'cleared to', valid_dataset_c.shape[0])\n",
    "print(test_dataset.shape[0], 'cleared to', test_dataset_c.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_dataset_c, test_labels_c = clear_overlap(\n",
    "    valid_dataset, valid_labels, test_dataset, test_labels,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_dataset_c, test_labels_c = clear_overlap(\n",
    "    train_dataset, train_labels, test_dataset_c, test_labels_c,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_dataset_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(valid_dataset.shape, valid_dataset_c.shape, valid_labels_c.shape)\n",
    "print(test_dataset.shape, test_dataset_c.shape, test_labels_c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check if classes are balanced\n",
    "from collections import Counter\n",
    "print(Counter(train_labels))\n",
    "print(Counter(valid_labels_c))\n",
    "print(Counter(test_labels_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train Logistic Regression Classifier on the dataset\n",
    "\n",
    "# Prepare data\n",
    "def prepare(dataset):\n",
    "    n, w, h = dataset.shape\n",
    "    return np.reshape(dataset,(n, w*h))\n",
    "\n",
    "X_train = prepare(train_dataset)\n",
    "y_train = train_labels\n",
    "\n",
    "X_test = prepare(test_dataset_c)\n",
    "y_test = test_labels_c\n",
    "\n",
    "model = LogisticRegression(\n",
    "    multi_class='multinomial', \n",
    "    solver='lbfgs', \n",
    "    random_state=42, \n",
    "    verbose=1, \n",
    "    max_iter=1000, \n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "metrics.accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
